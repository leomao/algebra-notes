%! TEX root=../main.tex
\section{Multilinear algebra}
\subsection{Week 11}
\subsubsection{Bilinear forms \& Groups preserving bilinear forms}

\begin{definition}
  Let $V$ be a vector space over a field $F$.
  \begin{itemize}
    \item A function $f: V \times V \to F$ is called a bilinear form if
      \[
        \begin{cases}
          f(rx_1 + x_2, y) &= rf(x_1, y) + f(x_2, y) \\
          f(x, ry_1 + y_2) &= rf(x, y_1) + f(x, y_2)
        \end{cases}
        \qquad \forall x_1, x_2, x, y_1, y_2, y \in V, r \in F
      \]
    \item $B_F(V, V) = \{\, \text{bilinear forms on $V$} \,\}$ can be regarded
      as a vector space over $F$.
  \end{itemize}
\end{definition}

\begin{theorem}
  Let $\dim V = n$ and $\beta = \{ v_1, \dots, v_n \}$ be a basis for $V$.
  Then $\exists$ an isomorphism $\psi_\beta: B_F(V, V) \to M_{n\times n}(F)$.
  \begin{proof}
    For $v, w \in V$, write $v = \sum_i a_iv_i, w = \sum_j b_j v_j$, i.e.
    $[v]_\beta = \begin{pmatrix}a_1\\ \vdots \\ a_n\end{pmatrix},
    [w]_\beta = \begin{pmatrix}b_1\\ \vdots \\ b_n\end{pmatrix}$.

    For $f \in B_F(V, V)$, $f(v, w) = \sum_i \sum_j a_ib_j f(v_i, v_j)
    = \begin{pmatrix}a_1 & \dots & a_n\end{pmatrix}
    \begin{pmatrix} \\ f(v_i, v_j) \\ \\ \end{pmatrix}
    \begin{pmatrix}b_1 \\ \vdots \\ b_n\end{pmatrix}$.

    Define $\psi_\beta(f) = A$ with $A_{ij} = f(v_i, v_j)$.
    \begin{itemize}
      \item $\psi_\beta$ is a linear transformation.
      \item $\psi_\beta$ is 1-1.
      \item $\psi_\beta$ is onto: $\forall A \in M_{n\times n}(F)$, we define
        $f(v, w) = [v]_\beta^t A [w]_\beta$.
        \qedhere
    \end{itemize}
  \end{proof}
\end{theorem}

\begin{definition}
  Let $f \in B_F(V, V)$
  \begin{itemize}
    \item $f$ is said to be symmetric if
      $f(v, w) = f(w, v) \quad \forall v, w \in V$.
    \item $f$ is said to be skew-symmetric if
      $f(v, w) = -f(w, v) \quad \forall v, w \in V$.
    \item $f$ is said to be alternating if $f(v, v) = 0 \quad \forall v \in V$.
  \end{itemize}
\end{definition}

\begin{remark} \mbox{}
  \begin{itemize}
    \item Alternating $\implies$ skew-symmetric.
    \item If $\Char F \ne 2$, skew-symmetric $\implies$ alternating.
    \item If $\Char F = 2$, symmetric $=$ skew-symmetric.
    \item $\forall f \in B_F(V, V)$ with $\Char F \ne 2$,
      \begin{align*}
        f_s(u, v) = \frac{1}{2}\left(f(u, v) + f(v, u)\right) \\
        f_a(u, v) = \frac{1}{2}\left(f(u, v) - f(v, u)\right)
      \end{align*}
      and $f(u, v) = f_s(u, v) + f_a(u, v)$.
  \end{itemize}
\end{remark}

So we only need to study ``symmetric'' \& ``alternating''.

\newpage

\begin{exercise} \mbox{}
  \begin{enumerate}
    \item If $A$ and $B$ are congruent $(B = Q^tAQ)$ in $M_{n\times n}(F)$,
      then they define the same bilinear form.
    \item $f$ is
      $\begin{cases}
        \text{symmetric} \\
        \text{skew-symmetric}
      \end{cases} \iff \psi_\beta(f) \text{~is~}
      \begin{cases}
        \text{symmetric} (A^t = A)\\
        \text{skew-symmetric} (A^t = -A)
      \end{cases}$
  \end{enumerate}
\end{exercise}

\begin{observation*}
  Let $f \in B_F(V, V)$ and $v_0 \in V$.
  \begin{align*}
    L_f(v_0) &= f(v_0, \cdot) \in V' = \Hom(V, F):
    \text{the dual space of $V$} \\
    R_f(v_0) &= f(\cdot, v_0) \in V'
  \end{align*}
\end{observation*}

The left radical of $f:
\Lrad(f) = N(L_f) = \{\, v\in V \mid f(v, w) = 0 \quad \forall w \in V \,\}$.

The right radical of $f:
\Rrad(f) = N(R_f) = \{\, w\in V \mid f(v, w) = 0 \quad \forall v \in V \,\}$.

\begin{exercise} \mbox{}
  \begin{enumerate}
    \item $\rank(\psi_\beta(f)) = \rank(R_f) = \rank(L_f)$.
    \item If $\dim V = n$, then TFAE ($\implies f:$ non degenerate)
      \begin{enumerate}[(a)]
        \item $\rank(f) = n$.
        \item $\forall v \in V, v \ne 0, \exists w \in V$ s.t. $f(v,w) \ne 0$.
        \item $\Lrad(f) = \{0\}$.
        \item $L_f: V \to V'$ is isom.
      \end{enumerate}
      (also, right)
  \end{enumerate}
\end{exercise}

\begin{theorem}[Principal Axis theorem]
  Let $\dim V = n$ and $\Char F \ne 2$.
  If $f \in B_F(V, V)$ is symmetric, then $\exists \beta$ s.t. $\psi_\beta(f)$
  is diagonal.

  \begin{proof}
    It is sufficient to find $\beta = \{v_1, \dots, v_n\}$ s.t.
    $f(v_i, v_j) = 0 \quad \forall i \ne j$.

    If $f = 0$, then done! Assume $f \ne 0$.
    By induction on $n$: If $n = 1$, done.
    Let $n > 1$.

    \underline{Claim 1}: $\exists v_1 \in V$ s.t. $f(v_1, v_1) \ne 0$.
    Assume that $f(v, v) = 0 \quad \forall v \in V$.
    %\[
      %f(v, w) = \frac{1}{4}f(v+w,v+w) - \frac{1}{4}f(v-w,v-w) = 0
    %\]
    {\color{red}
      \[ f(v, w) = \frac{1}{2} \big( f(v+w,v+w) - f(v, v) - f(w, w) \big) = 0. \quad \footnote{The argument in class requires $\Char F \geq 4$, omimi...} \] }
    So $f = 0$, which is a contradiction.

    Now let $v_1 \in V$ with $f(v_1, v_1) \ne 0$. Let $W = \gen{v_1}_F$ and
    $W^\perp = \{\, w \in V \mid f(v_1, w) = 0 \,\} \subseteq V$.

    \underline{Claim 2}: $V = W \oplus W^\perp$
    \begin{itemize}
      \item $V = W + W^\perp$: For all $v \in V$, let $a = f(v, v_1) / f(v_1, v_1)$, then $v = a v_1 + (v - a v_1)
        \triangleq w + w'$ where $w \in W$ and $f(w', v_1) = f(v - a v_1, v_1) = f(v, v_1) - a f(v_1, v_1)
        = 0$. So $w' \in W^\perp$ and thus $V = W + W^\perp$.
      \item $W \cap W^\perp = \{0\}$: obviously since if $a v_1 \in W$, $f(a v_1, v_1) = 0 \iff a = 0 \iff a v_1 = 0$.
    \end{itemize}

    Since $f \Big|_{W^\perp \times W^\perp}$ is a symmetric bilinear form
    on $W^\perp$ and $\dim W^\perp < \dim V$.
    By induction hypothesis, $\exists \{ v_2, \dots, v_n \}$ a basis for
    $W^\perp$ s.t. $f(v_i, v_j) = 0 \quad \forall i \ne j$. Then
    $\beta = \{v_1, \dots, v_n \}$.
  \end{proof}
\end{theorem}

\begin{theorem}[Sylvester's theorem]
  Let $f \in B_\Rb(V, V)$ be symmetric with $\dim V = n$. Then $\exists \beta$
  s.t. $\psi_\beta(f) = \begin{pmatrix}
    1 \\
    & \ddots \\
    & & 1 \\
    & & & -1 \\
    & & & & \ddots \\
    & & & & & -1 \\
    & & & & & & 0 \\
    & & & & & & & \ddots \\
    & & & & & & & & 0 \\
  \end{pmatrix}$.

  The triple (\# of 1, \# of -1, \# of 0) is well-defined.
  (called the signature of $f$)
  \begin{proof}


    Assume $V^+ = \gen{v_1, \dots, v_p}_F, V^- = \gen{v_{p+1}, \dots, v_{r}}_R,
    V^\perp = \gen{v_{r+1}, \dots, v_n}_F$. ($V = V^+ \oplus V^- \oplus V^\perp$)

    Claim: If $W$ is a subspace of $V$ s.t. $f$ is positive-definite on $W$,
    then $W, V^-, V^\perp$ are independent.

    Let $\gen{w_1, w_2, \cdots, w_s}$ be a basis of $W$. If
    \[ a_1 w_1 + a_2 w_2 + \cdots + a_s w_s = b_{p+1} v_{p+1} + \cdots + b_r v_r + 
      c_{r+1} v_{r+1} + \cdots + c_n v_n. \]
    Let $w \triangleq a_1 w_1 + \cdots + a_s w_s, v \triangleq b_{p+1} v_{p+1} + \cdots + b_r v_r + 
    c_{r+1} v_{r+1} + \cdots + c_n v_n$. Since $w = v$, $f(w, w) = f(v, v)$. 
    but $f(w, w) = \sum a_i^2 \geq 0$ and $f(v, v) = - \sum b_i^2 \leq 0$. Hence $a_i = 0, b_i = 0$.
    Since $v_{r+1}, \cdots, v_n$ is linear independent, $c_i = 0$. Therefor these vectors are linear 
    independent.

  \end{proof}
\end{theorem}

\begin{exercise}
  Let $f \in B_F(V, V)$ with $\Char F \ne 2$.
  If $f$ is skew-symmetric, then $\exists \beta$ s.t.
  \[
    \psi_\beta(f) = \begin{pmatrix}
      0 & 1 \\
      -1 & 0 \\
      & & 0 & 1 \\
      & & -1 & 0 \\
      & & & & \ddots \\
      & & & & & 0 & 1 \\
      & & & & & -1 & 0 \\
      & & & & & & & 0 \\
      & & & & & & & & \ddots \\
      & & & & & & & & & 0
    \end{pmatrix}
  \]
\end{exercise}

\begin{exercise}
  Study Hermitian form
\end{exercise}

${\sf T}: V \isoto V, f\in B_F(V, V)$.
${\sf T}$ preserves $f$ if $f({\sf T}(v), {\sf T}(w)) = f(v, w) \quad
\forall v,w \in V$.

In matrix form, let $\beta$ be a basis for $V$,
$M =[{\sf T}]_\beta, A = \psi_\beta(f)$, then $A = M^tAM$.

\begin{itemize}
  \item $f \in B_\Rb(V,V)$ symmetric, non-degenerate:
    $\exists \beta$ s.t. $\psi_\beta(f) = \begin{pmatrix}I_p \\ & -I_q\end{pmatrix}$.

    Then $\{\, {\sf T}: V \isoto V \text{~preserves~} f \,\} \leftrightarrow
    \left\{\,
      M \in \text{GL}_n(\Rb) \middle|
      M^t\begin{pmatrix}I_p \\ & -I_q\end{pmatrix}M = \begin{pmatrix}I_p \\ & -I_q\end{pmatrix}
      \,\right\} = \text{O}(p, q)$.
  \item $f \in B_\Rb(V,V)$ skew-symmetric, non-degenerate: $n = 2k$,
    $\exists \beta$ s.t. $\psi_\beta(f) = J$.

    Then $\{\, {\sf T}: V \isoto V \text{~preserves~} f \,\} \leftrightarrow
    \left\{\,
      M \in \text{GL}_n(\Rb) \middle|
      M^tJM = J
      \,\right\}$, where 
    \[ J = \begin{pmatrix}
        0 & I_k \\
        -I_k & 0
    \end{pmatrix} \]
\end{itemize}

\subsubsection{Tensor product}
From now on, $R$ is assumed to be commutative with $1$.

\begin{definition}
Let $M_1, \dots, M_n, L$ be $R$-modules.

A function $F: M_1 \times \dots \times M_n \to L$ is said to be $n$-multilinear
if $\forall i$,
\[
  f(x_1, \dots, rx_i + x_i', \dots, x_n) =
  rf(x_1, \dots, x_i, \dots, x_n) + f(x_1, \dots, x_i', \dots, x_n)
  \quad \forall r \in R, x_i, x_i' \in M_i
\]
If $n = 2$, $f$ is called a bilinear map.
\end{definition}

\begin{definition}
  Let $M, N$ be $R$-modules. A tensor product of $M$ and $N$ is an $R$-module
  $M \otimes_R N$ with a bilinear map $\rho: M\times N \to M\otimes_R N$
  satisfying the following universal property:

  for any $R$-momdule $W$ and any bilinear map $f: M\times N \to W$,
  $\exists!$ $R$-module homomorphism $\varphi: M \otimes_R N \to W$,
  \[
    \begin{tikzcd}
    M\times N \arrow{r}{\rho} \arrow[swap]{dr}{f}
      & M \otimes_R N \arrow{d}{\varphi} \\
    & W
    \end{tikzcd}
  \]
\end{definition}

\begin{theorem}[Main theorem]
  $M \otimes_R N$ exists and is unique up to isom.
  \begin{proof}
    Let $X = M\times N$.
    First we construct the free module
    $\displaystyle V_1 = \bigoplus_{(x, y) \in X} R \cdot (x, y)$. \\
    Notice that in $V_1$,
    \begin{itemize}
      \item $(x_1, y_1) + (x_2, y_2) \ne (x_1+x_2, y_1+y_2)$.
      \item $r(x, y) \ne (rx, ry)$.
      \item $r(r_1(x_1, y_1) + \dots + r_n(x_n, y_n)) =
        rr_1(x_1, y_1) + \dots + rr_n(x_n, y_n)$.
    \end{itemize}
    Let $V_0 = \gen*{
      \begin{gathered}
        (x_1+x_2, y) - (x_1, y) - (x_2, y), \\
        (x, y_1+y_2) - (x, y_1) - (x, y_2), \\
        r(x, y) - (rx, y), r(x, y) - (x, ry)
      \end{gathered}
      \middle| x_1, x_2, x \in M, y_1, y_2, y \in N, r \in R
    }_R$.

    Define $M \otimes_R N = \quot{V_1}{V_0}$ which is an $R$-module and
    $\arraycolsep=1pt
    \begin{array}{rcl}
      \rho: M \times N & \to & M \otimes_R N \\
      (x, y) & \mapsto & (x, y) + V_0 = x \otimes y
    \end{array}$
    which is $R$-bilinear. (check yourself)

    Universal property: $\forall (x, y) \in M \times N$,
    $\arraycolsep=1pt
    \begin{array}{rcl}
      R(x, y) & \to & W \\
      r(x, y) & \mapsto & rf(x, y)
    \end{array}$. So, by the universal property of $\oplus$,
    $\exists!$ $R$-module homo. $\varphi_1: V_1 \to W$:
    \[
      \begin{tikzcd}
      M\times N \arrow{r}{i} \arrow[swap]{dr}{f}
        & V_1 \arrow{d}{\varphi_1} \\
      & W
      \end{tikzcd}
    \]
    Claim: $V_0 \subseteq \ker \varphi_1$. (check yourself)
    Then by factor theorem,
    \[
      \begin{tikzcd}[cramped, column sep=tiny]
        \exists! \varphi: \quot{V_1}{V_0} \arrow{rr} & & W \\
        & M \times N \arrow{ul} \arrow{ur} &
      \end{tikzcd}
      \qedhere
    \]
  \end{proof}
\end{theorem}

\begin{example}
  $\Qb \otimes_\Zb \quot{\Zb}{n\Zb} = 0$.
\end{example}

\begin{example}
  $\Rb[x, y] \cong \Rb[x] \otimes_\Rb \Rb[y]$.
  \begin{proof}
    $\arraycolsep=1pt
    \begin{array}{rcl}
      \Rb[x] \times \Rb[y] & \to & \Rb[x, y] \\
      ( f(x), g(y) ) & \mapsto & f(x)g(y)
    \end{array}$
    is bilinear $\leadsto$
    $\arraycolsep=1pt
    \begin{array}{rcl}
      \exists! \varphi: \Rb[x] \otimes_\Rb \Rb[y] & \to & \Rb[x, y] \\
       f(x) \otimes g(y) & \mapsto & f(x)g(y)
    \end{array}$.

    Conversely, 
    $\arraycolsep=1pt
    \begin{array}{rcl}
      \Rb[x, y] & \to & \Rb[x] \otimes_\Rb \Rb[y] \\
      h(x, y) = \sum a_{ij} x^i y^j & \mapsto & \sum a_{ij} x_i \otimes y_j
    \end{array}$.
  \end{proof}
\end{example}

\begin{prop}
  If $M = \gen{x_1, \dots, x_n}_R$ and $N = \gen{y_1, \dots, y_m}_R$. Then
  \[ M \otimes_R N = \gen{x_i \otimes y_j \mid i = 1, \dots, n;
  j = 1, \dots, m}_R. \]
  In particular, if $R$ is a field $F$, then
  $\dim_F M\otimes_F N = (\dim_F M)(\dim_F N)$.
  \begin{proof}
    Note that $M \otimes_R N = \gen{x \otimes y \mid x \in M, y \in N}$.
    Let $x = \sum_i a_ix_i, y = \sum_j b_jy_j$. Then
    $x\otimes y = \sum_i \sum_j a_ib_j x_i \otimes y_j$.
  \end{proof}
\end{prop}

Some canonical isomorphisms:
\begin{itemize}
  \item $(M \otimes_R N) \otimes_R L \cong M \otimes_R (N \otimes_R L)$.
    \begin{proof}
      $\forall z \in L$, 
      $\arraycolsep=1pt
      \begin{array}{rcl}
        M \times N & \to & M \otimes_R (N \otimes_R L) \\
        (x, y) & \mapsto & x \otimes (y \otimes z)
      \end{array}$ is bilinear.
      $\exists!$ $R$-mod homo.
      $\varphi_z: M \otimes_R N \to M\otimes_R(N\otimes_R L)$.
      Similarly,
      $\arraycolsep=1pt
      \begin{array}{rcl}
        (M \otimes_R N) \times L & \to & M \otimes_R (N \otimes_R L) \\
        \left(\sum x_i\otimes y_i, z\right) & \mapsto &
        \sum x_i \otimes (y_i \otimes z)
      \end{array}$ is bilinear. (The right is due to $\varphi_z$ linear, and 
      the left is because $x \otimes (y \otimes (rz_1 + z_2)) = rx \otimes (y \otimes z_1) + 
      x \otimes (y \otimes z_2)$.)
      Hence exists unique $R$-mod homo.
      $\varphi: (M \otimes_R N) \otimes_R L \to M \otimes_R (N \otimes_R L)$.
      By the symmetric construction, we have $\varphi^{-1}$ and $\varphi^{-1} \circ 
      \varphi = \varphi \circ \varphi^{-1} = 1$, so the two are isomorphic.
    \end{proof}
  \item $(M \oplus M') \otimes_R N \cong (M \otimes_R N)\oplus(M'\otimes_R N)$.

    The mapping $\psi :: (M \oplus M') \times N \to (M \otimes_R N)\oplus(M'\otimes_R N)$
    by $\psi = ((x, x'), y) \mapsto (x\otimes y, x'\otimes y)$ is biliear, hence 
    exists R-mod homomorphism $\varphi :: (M \oplus M') \otimes_R N \to (M \otimes_R N)\oplus(M'\otimes_R N)$.

    On the other hand, The mapping $(x, y) :: M \times N \mapsto (x, 0) \otimes y :: (M \oplus M') \otimes_R N$ 
    is bilinear. So exists $\phi_1 :: M \otimes N \to (M \oplus M') \otimes_R N$, similarly there exists
    $\phi_2 :: M' \otimes N \to (M \oplus M') \otimes_R N$. Now by the universal property of direct sum, 
    there exists $\phi :: (M \otimes_R N)\oplus(M'\otimes_R N) \to (M \oplus M') \otimes_R N$.
    After a careful examine, we have
    \[ \varphi = (x, x') \otimes y \mapsto (x \otimes y, x' \otimes y),
      \phi = (x \otimes y, x' \otimes y) \mapsto (x, x') \otimes y \]
    Thus $\phi = \varphi^{-1}$ and hence the two are isomorphic.
\end{itemize}

\begin{exercise}\mbox{}
  \begin{enumerate}
    \item $R \otimes_R M \cong M$.
    \item $M \otimes_R N \cong N \otimes_R M$.
  \end{enumerate}
\end{exercise}

\begin{exercise}
  $\quot{R}{I} \otimes_R N \cong \quot{N}{IN}$ where
  $IN \defeq \left\{ \sum a_ix_i \mid a_i \in I, x_i \in N \right\}$.
\end{exercise}

\begin{exercise}
  Compute
  $
  \dim_\Qb (\Qb \otimes_\Zb \Qb), 
  \dim_\Rb (\Rb \otimes_\Rb \Cb), 
  \dim_\Rb (\Cb \otimes_\Rb \Cb), 
  \dim_\Cb (\Cb \otimes_\Rb \Cb)
  $
\end{exercise}
